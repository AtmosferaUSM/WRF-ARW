{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Create Cluster Link to AWS Link to PCluster Manager Create Cluster Upload YML file HeadNode: InstanceType: c5a.xlarge Ssh: KeyName: PClusterManager Networking: SubnetId: subnet-0412e7315562aa1da LocalStorage: RootVolume: VolumeType: gp3 Size: 50 Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Dcv: Enabled: true Scheduling: Scheduler: slurm SlurmQueues: - Name: queue0 ComputeResources: - Name: queue0-hpc6a48xlarge MinCount: 0 MaxCount: 10 InstanceType: hpc6a.48xlarge Efa: Enabled: true GdrSupport: true DisableSimultaneousMultithreading: true Networking: SubnetIds: - subnet-01a39ae1f7194644a PlacementGroup: Enabled: true ComputeSettings: LocalStorage: RootVolume: VolumeType: gp3 Size: 50 Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Region: us-east-2 Image: Os: alinux2 SharedStorage: - Name: Ebs0 StorageType: Ebs MountDir: /shared EbsSettings: VolumeType: gp3 DeletionPolicy: Delete Size: '200' Encrypted: false Create Cluster Dry Run Request would have succeeded, but DryRun flag is set. Request would have succeeded, but DryRun flag is set. Create Create in Progress Create Complete Click on Shell Login to your AWS account","title":"Create Cluster"},{"location":"#create-cluster","text":"Link to AWS Link to PCluster Manager Create Cluster","title":"Create Cluster"},{"location":"#upload-yml-file","text":"HeadNode: InstanceType: c5a.xlarge Ssh: KeyName: PClusterManager Networking: SubnetId: subnet-0412e7315562aa1da LocalStorage: RootVolume: VolumeType: gp3 Size: 50 Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Dcv: Enabled: true Scheduling: Scheduler: slurm SlurmQueues: - Name: queue0 ComputeResources: - Name: queue0-hpc6a48xlarge MinCount: 0 MaxCount: 10 InstanceType: hpc6a.48xlarge Efa: Enabled: true GdrSupport: true DisableSimultaneousMultithreading: true Networking: SubnetIds: - subnet-01a39ae1f7194644a PlacementGroup: Enabled: true ComputeSettings: LocalStorage: RootVolume: VolumeType: gp3 Size: 50 Iam: AdditionalIamPolicies: - Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Region: us-east-2 Image: Os: alinux2 SharedStorage: - Name: Ebs0 StorageType: Ebs MountDir: /shared EbsSettings: VolumeType: gp3 DeletionPolicy: Delete Size: '200' Encrypted: false","title":"Upload YML file"},{"location":"#create-cluster_1","text":"Dry Run Request would have succeeded, but DryRun flag is set. Request would have succeeded, but DryRun flag is set. Create","title":"Create Cluster"},{"location":"#create-in-progress","text":"","title":"Create in Progress"},{"location":"#create-complete","text":"Click on Shell Login to your AWS account","title":"Create Complete"},{"location":"geogrid/","text":"Geogrid Preliminary Before we start executing geogrid.exe, ungrib.exe and metgrid.exe, create a folder named WRF_Resources to put all the required input data for WPS. We will download all the required data under the /scratch folder. mkdir WRF_Resources cd WRF_Resources Download the geogrid data You can download the mandatory high resolution data from the official website of UCAR: Download data for the input of geogrid.There are two types of data sets available, each made up of the hightest and lowest resolution of each mandatory field. You may read the details on the official website to decide on the data that suits your need. In this case, we will be using the highest resolution of each mandatory field. wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz tar -xf geog_high_res_mandatory.tar.gz Point the directory of high resolution data to WRF_Resources in the nameslist. To do that, you will first have to obtain the path to WPS_GEOG. cd WPS_GEOG pwd Copy the path and go to the directory /shared/scratch/WPS/WRF_Resources/WPS_GEOG/ where your WPS is compiled and edit the namelist.wps. cd /shared/scratch/WPS/ nano namelist.wps Under the &geogrid section, edit the geog_data_path and save it. *screenshot Export the LD_LIBRARY_PATH and run the geogrid.exe. Make sure you had load the intel-oneapi-compilers and intel-oneapi-mpi using spack to avoid issue missing libiomp5.so. export LD_LIBRARY_PATH=$(spack location -i netcdf-fortran%intel)/lib/ spack load intel-oneapi-compilers spack load intel-oneapi-mpi ./geogrid.exe If your geogrid runs successfully, the following output will be printed. *screenshot","title":"GEOGRID"},{"location":"geogrid/#geogrid","text":"","title":"Geogrid"},{"location":"geogrid/#preliminary","text":"Before we start executing geogrid.exe, ungrib.exe and metgrid.exe, create a folder named WRF_Resources to put all the required input data for WPS. We will download all the required data under the /scratch folder. mkdir WRF_Resources cd WRF_Resources","title":"Preliminary"},{"location":"geogrid/#download-the-geogrid-data","text":"You can download the mandatory high resolution data from the official website of UCAR: Download data for the input of geogrid.There are two types of data sets available, each made up of the hightest and lowest resolution of each mandatory field. You may read the details on the official website to decide on the data that suits your need. In this case, we will be using the highest resolution of each mandatory field. wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz tar -xf geog_high_res_mandatory.tar.gz Point the directory of high resolution data to WRF_Resources in the nameslist. To do that, you will first have to obtain the path to WPS_GEOG. cd WPS_GEOG pwd Copy the path and go to the directory /shared/scratch/WPS/WRF_Resources/WPS_GEOG/ where your WPS is compiled and edit the namelist.wps. cd /shared/scratch/WPS/ nano namelist.wps Under the &geogrid section, edit the geog_data_path and save it. *screenshot Export the LD_LIBRARY_PATH and run the geogrid.exe. Make sure you had load the intel-oneapi-compilers and intel-oneapi-mpi using spack to avoid issue missing libiomp5.so. export LD_LIBRARY_PATH=$(spack location -i netcdf-fortran%intel)/lib/ spack load intel-oneapi-compilers spack load intel-oneapi-mpi ./geogrid.exe If your geogrid runs successfully, the following output will be printed. *screenshot","title":"Download the geogrid data"},{"location":"intel_compiler/","text":"Intel Compiler Installation spack install --no-cache intel-oneapi-compilers@2022.0.2 -- spack find -- spack load intel-oneapi-compilers spack compiler find spack unload -- spack compilers Install Intel MPI spack install --no-cache intel-oneapi-mpi+external-libfabric%intel","title":"Intel Compiler Installation"},{"location":"intel_compiler/#intel-compiler-installation","text":"spack install --no-cache intel-oneapi-compilers@2022.0.2 -- spack find -- spack load intel-oneapi-compilers spack compiler find spack unload -- spack compilers","title":"Intel Compiler Installation"},{"location":"intel_compiler/#install-intel-mpi","text":"spack install --no-cache intel-oneapi-mpi+external-libfabric%intel","title":"Install Intel MPI"},{"location":"metgrid/","text":"METGRID Export the library path export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib Load the compilers spack load intel-oneapi-compilers spack load intel-oneapi-mpi Run metgird. ./metgrid.exe","title":"METGRID"},{"location":"metgrid/#metgrid","text":"","title":"METGRID"},{"location":"metgrid/#export-the-library-path","text":"export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib","title":"Export the library path"},{"location":"metgrid/#load-the-compilers","text":"spack load intel-oneapi-compilers spack load intel-oneapi-mpi Run metgird. ./metgrid.exe","title":"Load the compilers"},{"location":"namelist/","text":"Resources namelist.input cat <<EOF > namelist.input &time_control run_days = 0, run_hours = 18, run_minutes = 0, run_seconds = 0, start_year = 2021, start_month = 06, start_day = 29, start_hour = 00, start_minute = 00, start_second = 00, end_year = 2021, end_month = 06, end_day = 29, end_hour = 18, end_minute = 00, end_second = 00, interval_seconds = 21600 input_from_file = .true., history_interval = 60, frames_per_outfile = 1, restart = .false., restart_interval = 10000, io_form_history = 2, io_form_restart = 2, io_form_input = 2, io_form_boundary = 2, / &domains perturb_input = .false. time_step = 10, time_step_fract_num = 0, time_step_fract_den = 1, max_dom = 1, s_we = 1, e_we = 100, e_sn = 127, e_vert = 45, dx = 15000, dy = 15000, grid_id = 1, parent_id = 1, i_parent_start = 1, j_parent_start = 1, parent_grid_ratio = 1, parent_time_step_ratio = 1, feedback = 1, smooth_option = 0 num_metgrid_levels = 34, num_metgrid_soil_levels = 4, / &physics physics_suite = 'tropical' mp_physics = 6, ra_lw_physics = 4, ra_sw_physics = 4, radt = 30, sf_sfclay_physics = 91, sf_surface_physics = 2, bl_pbl_physics = 1, bldt = 0, cu_physics = 16, cudt = 0, isfflx = 1, ifsnow = 0, icloud = 1, surface_input_source = 1, num_soil_layers = 1, maxiens = 1, maxens = 1, maxens2 = 1, maxens3 = 1, ensdim = 1, / &dynamics w_damping = 0, diff_opt = 2, km_opt = 4, khdif = 0, kvdif = 0, non_hydrostatic = .true., moist_adv_opt = 1, scalar_adv_opt = 1, use_baseparam_fr_nml = .t. / &bdy_control spec_bdy_width = 5, spec_zone = 1, relax_zone = 4, specified = .true., nested = .false., / &namelist_quilt nio_tasks_per_group = 0, nio_groups = 1, / EOF namelist.wps cat <<EOF > namelist.wps &share wrf_core = 'ARW', max_dom = 1, start_date = '2021-06-30_00:00:00','2021-06-30_00:00:00', end_date = '2021-06-30_18:00:00','2021-06-30_18:00:00', interval_seconds = 21600 / &geogrid parent_id = 1, 1, parent_grid_ratio = 1, 3, i_parent_start = 1, 53, j_parent_start = 1, 25, e_we = 100, 220, e_sn = 127, 214, geog_data_res = 'default','default', dx = 4100, dy = 4100, map_proj = 'mercator', ref_lat = 5.464, ref_lon = 100.289, truelat1 = 5.464, truelat2 = 0, stand_lon = 100.261, geog_data_path = '/shared/scratch/WRF_Resources/WPS_GEOG/' / &ungrib out_format = 'WPS', prefix = 'FILE', / &metgrid fg_name = 'FILE' / EOF","title":"Resources"},{"location":"namelist/#resources","text":"","title":"Resources"},{"location":"namelist/#namelistinput","text":"cat <<EOF > namelist.input &time_control run_days = 0, run_hours = 18, run_minutes = 0, run_seconds = 0, start_year = 2021, start_month = 06, start_day = 29, start_hour = 00, start_minute = 00, start_second = 00, end_year = 2021, end_month = 06, end_day = 29, end_hour = 18, end_minute = 00, end_second = 00, interval_seconds = 21600 input_from_file = .true., history_interval = 60, frames_per_outfile = 1, restart = .false., restart_interval = 10000, io_form_history = 2, io_form_restart = 2, io_form_input = 2, io_form_boundary = 2, / &domains perturb_input = .false. time_step = 10, time_step_fract_num = 0, time_step_fract_den = 1, max_dom = 1, s_we = 1, e_we = 100, e_sn = 127, e_vert = 45, dx = 15000, dy = 15000, grid_id = 1, parent_id = 1, i_parent_start = 1, j_parent_start = 1, parent_grid_ratio = 1, parent_time_step_ratio = 1, feedback = 1, smooth_option = 0 num_metgrid_levels = 34, num_metgrid_soil_levels = 4, / &physics physics_suite = 'tropical' mp_physics = 6, ra_lw_physics = 4, ra_sw_physics = 4, radt = 30, sf_sfclay_physics = 91, sf_surface_physics = 2, bl_pbl_physics = 1, bldt = 0, cu_physics = 16, cudt = 0, isfflx = 1, ifsnow = 0, icloud = 1, surface_input_source = 1, num_soil_layers = 1, maxiens = 1, maxens = 1, maxens2 = 1, maxens3 = 1, ensdim = 1, / &dynamics w_damping = 0, diff_opt = 2, km_opt = 4, khdif = 0, kvdif = 0, non_hydrostatic = .true., moist_adv_opt = 1, scalar_adv_opt = 1, use_baseparam_fr_nml = .t. / &bdy_control spec_bdy_width = 5, spec_zone = 1, relax_zone = 4, specified = .true., nested = .false., / &namelist_quilt nio_tasks_per_group = 0, nio_groups = 1, / EOF","title":"namelist.input"},{"location":"namelist/#namelistwps","text":"cat <<EOF > namelist.wps &share wrf_core = 'ARW', max_dom = 1, start_date = '2021-06-30_00:00:00','2021-06-30_00:00:00', end_date = '2021-06-30_18:00:00','2021-06-30_18:00:00', interval_seconds = 21600 / &geogrid parent_id = 1, 1, parent_grid_ratio = 1, 3, i_parent_start = 1, 53, j_parent_start = 1, 25, e_we = 100, 220, e_sn = 127, 214, geog_data_res = 'default','default', dx = 4100, dy = 4100, map_proj = 'mercator', ref_lat = 5.464, ref_lon = 100.289, truelat1 = 5.464, truelat2 = 0, stand_lon = 100.261, geog_data_path = '/shared/scratch/WRF_Resources/WPS_GEOG/' / &ungrib out_format = 'WPS', prefix = 'FILE', / &metgrid fg_name = 'FILE' / EOF","title":"namelist.wps"},{"location":"ncl/","text":"NCL Installation Install NCL spack install ncl^hdf5@1.8.22 -- spack load ncl ncl -h -- cat << EOF > $HOME/.hluresfile *windowWorkstationClass*wkWidth : 1000 *windowWorkstationClass*wkHeight : 1000 EOF","title":"NCL Installation"},{"location":"ncl/#ncl-installation","text":"","title":"NCL Installation"},{"location":"ncl/#install-ncl","text":"spack install ncl^hdf5@1.8.22 -- spack load ncl ncl -h -- cat << EOF > $HOME/.hluresfile *windowWorkstationClass*wkWidth : 1000 *windowWorkstationClass*wkHeight : 1000 EOF","title":"Install NCL"},{"location":"run_wrf/","text":"Running WRF Congratulations on your successful running of WPS. It is time to run WRF. Go into the WRF/run directory and link met_em* file cd $(spack location -i wrf%intel)/run ln -sf /shared/scratch/WPS/met_em* . ls -alh Edit the namelist.input and make necessary changes. The details including &time_control and &domains has to be the same as defined in the namelist.wps .You may change the &physics option here too. Refer to UCAR Website for the best practices. nano namelist.input Execute the real program mpirun -np 1 ./real.exe Check the rsl.error files to ensure that the run was successful. To do that, you can use the tail command. Usually, the rsl.error0000 will contain the most information.This indicate the successful run of real.exe real_em: SUCCESS COMPLETE REAL_EM INIT tail rsl.error.0000 Now you are ready to run the wrf.exe . We can use the slurm to submit and distribute the tasks for us.Kindly amend the information according to your need. cat <<EOF > slurm-wrf-penang.sh #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=penang-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np $SLURM_NTASKS --ppn $SLURM_NTASKS_PER_NODE $wrf_exe EOF Submit the slurm job. You can check the run by squeue . sbatch slurm-wrf-penang.sh Check the rsl.out.0000 file to see if the run was successful. You will see a successful message printed at the end of the file. tail rsl.out.0000","title":"Running WRF"},{"location":"run_wrf/#running-wrf","text":"Congratulations on your successful running of WPS. It is time to run WRF. Go into the WRF/run directory and link met_em* file cd $(spack location -i wrf%intel)/run ln -sf /shared/scratch/WPS/met_em* . ls -alh Edit the namelist.input and make necessary changes. The details including &time_control and &domains has to be the same as defined in the namelist.wps .You may change the &physics option here too. Refer to UCAR Website for the best practices. nano namelist.input Execute the real program mpirun -np 1 ./real.exe Check the rsl.error files to ensure that the run was successful. To do that, you can use the tail command. Usually, the rsl.error0000 will contain the most information.This indicate the successful run of real.exe real_em: SUCCESS COMPLETE REAL_EM INIT tail rsl.error.0000 Now you are ready to run the wrf.exe . We can use the slurm to submit and distribute the tasks for us.Kindly amend the information according to your need. cat <<EOF > slurm-wrf-penang.sh #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=penang-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np $SLURM_NTASKS --ppn $SLURM_NTASKS_PER_NODE $wrf_exe EOF Submit the slurm job. You can check the run by squeue . sbatch slurm-wrf-penang.sh Check the rsl.out.0000 file to see if the run was successful. You will see a successful message printed at the end of the file. tail rsl.out.0000","title":"Running WRF"},{"location":"spack/","text":"Spack Installation Verify the Cluster df -h -- sinfo Spack Installation Download Spack export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT Define .bashrc file echo \"export SPACK_ROOT=/shared/spack\" >> $HOME/.bashrc echo \"source \\$SPACK_ROOT/share/spack/setup-env.sh\" >> $HOME/.bashrc source $HOME/.bashrc Spack Version spack -V 0.18.1 (13e6187e\u00a365279546152eaea303841978e836992) Patchelf Installation -- spack install patchelf -- spack find spack load patchelf which patchelf Spack build cache pip3 install botocore==1.23.46 boto3==1.20.46 Add the mirror to the binary cache spack mirror add aws-hpc-weather s3://aws-hpc-weather/spack/ spack buildcache keys --install --trust --force Adding parallel cluster's softwares to the Spack cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False EOF","title":"Spack Installation"},{"location":"spack/#spack-installation","text":"","title":"Spack Installation"},{"location":"spack/#verify-the-cluster","text":"df -h -- sinfo","title":"Verify the Cluster"},{"location":"spack/#spack-installation_1","text":"Download Spack export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT Define .bashrc file echo \"export SPACK_ROOT=/shared/spack\" >> $HOME/.bashrc echo \"source \\$SPACK_ROOT/share/spack/setup-env.sh\" >> $HOME/.bashrc source $HOME/.bashrc Spack Version spack -V 0.18.1 (13e6187e\u00a365279546152eaea303841978e836992) Patchelf Installation -- spack install patchelf -- spack find spack load patchelf which patchelf Spack build cache pip3 install botocore==1.23.46 boto3==1.20.46 Add the mirror to the binary cache spack mirror add aws-hpc-weather s3://aws-hpc-weather/spack/ spack buildcache keys --install --trust --force Adding parallel cluster's softwares to the Spack cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False EOF","title":"Spack Installation"},{"location":"test_wrf/","text":"Testing WRF Load WRF spack load wrf Find WRF location ls `spack location -i wrf`/run/wrf.exe -- cd /shared mkdir scratch cd scratch -- curl -O https://www2.mmm.ucar.edu/wrf/OnLineTutorial/wrf_cloud/wrf_simulation_CONUS12km.tar.gz tar -xzf wrf_simulation_CONUS12km.tar.gz -- cd conus_12km WRF_ROOT=$(spack location -i wrf%intel)/test/em_real/ ln -s $WRF_ROOT* . -- cat <<EOF > slurm-wrf-conus12km.sh #!/bin/bash #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=conus-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=\\$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe EOF -- sbatch slurm-wrf-conus12km.sh -- DCV cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/surface.ncl","title":"Testing WRF"},{"location":"test_wrf/#testing-wrf","text":"","title":"Testing WRF"},{"location":"test_wrf/#load-wrf","text":"spack load wrf Find WRF location ls `spack location -i wrf`/run/wrf.exe -- cd /shared mkdir scratch cd scratch -- curl -O https://www2.mmm.ucar.edu/wrf/OnLineTutorial/wrf_cloud/wrf_simulation_CONUS12km.tar.gz tar -xzf wrf_simulation_CONUS12km.tar.gz -- cd conus_12km WRF_ROOT=$(spack location -i wrf%intel)/test/em_real/ ln -s $WRF_ROOT* . -- cat <<EOF > slurm-wrf-conus12km.sh #!/bin/bash #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=conus-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=\\$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe EOF -- sbatch slurm-wrf-conus12km.sh --","title":"Load WRF"},{"location":"test_wrf/#dcv","text":"cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/surface.ncl","title":"DCV"},{"location":"ungrib/","text":"UNGRIB Create GFS data under /WRF_Resources that you just created. cd /shared/scratch/WPS/WRF_Resources mkdir GFS_Data cd GFS_Data Website description, data and check if your dates are available. Create a python script to download the GFS Data. Make necessary changes on the dates and hours.This python script downloads 4 data on 20210629 with 6 hours interval. The alternatives would be going to the official Research Data Archive website : description and upload the python script through DCV. cat <<EOF > download_20210629.py #!/usr/bin/env python ################################################################# # Python Script to retrieve 4 online Data files of 'ds084.1', # total 2.06G. This script uses 'requests' to download data. # # Highlight this script by Select All, Copy and Paste it into a file; # make the file executable and run it on command line. # # You need pass in your password as a parameter to execute # this script; or you can set an environment variable RDAPSWD # if your Operating System supports it. # # Contact rdahelp@ucar.edu (RDA help desk) for further assistance. ################################################################# import sys, os import requests def check_file_status(filepath, filesize): sys.stdout.write('\\r') sys.stdout.flush() size = int(os.stat(filepath).st_size) percent_complete = (size/filesize)*100 sys.stdout.write('%.3f %s' % (percent_complete, '% Completed')) sys.stdout.flush() # Try to get password if len(sys.argv) < 2 and not 'RDAPSWD' in os.environ: try: import getpass input = getpass.getpass except: try: input = raw_input except: pass pswd = input('Password: ') else: try: pswd = sys.argv[1] except: pswd = os.environ['RDAPSWD'] url = 'https://rda.ucar.edu/cgi-bin/login' values = {'email' : 'YOUR EMAIL', 'passwd' : pswd, 'action' : 'login'} # Authenticate ret = requests.post(url,data=values) if ret.status_code != 200: print('Bad Authentication') print(ret.text) exit(1) dspath = 'https://rda.ucar.edu/data/ds084.1/' filelist = [ '2021/20210629/gfs.0p25.2021062900.f000.grib2', '2021/20210629/gfs.0p25.2021062906.f000.grib2', '2021/20210629/gfs.0p25.2021062912.f000.grib2', '2021/20210629/gfs.0p25.2021062918.f000.grib2'] for file in filelist: filename=dspath+file file_base = os.path.basename(file) print('Downloading',file_base) req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True) filesize = int(req.headers['Content-length']) with open(file_base, 'wb') as outfile: chunk_size=1048576 for chunk in req.iter_content(chunk_size=chunk_size): outfile.write(chunk) if chunk_size < filesize: check_file_status(file_base, filesize) check_file_status(file_base, filesize) print() EOF Execute the python script to download the data. You must be a registered user on the NCAR website because you will be asked for password before the permission to download the data is granted. pip3 install requests python3 download_20210629.py Obtain the path to your GFS_Data where in this case and go back to the directory where your WPS is compiled and create the symbolic link to the GFS data that you just created. You should expect four GRIBFILE with same prefix but different labels behind, AAA, AAB,AAC and AAD. cd /shared/scratch/WPS ./link_grib.csh /shared/scratch/WPS/WRF_Resources/GFS_Data/gfs.* *screenshot for the four output GRIBFILE.AAA Create the symbolic link to the Vtable. ln -sf ungrib/Variable_Tables/Vtable.GFS Vtable *screenshot for the Vtable Edit the namelist.wps. start date end date the interval seconds that desribe the interval between your GFS data -- nano namelist.wps Change the domain to 1 or 2 according to your needs. Increase your stack limit as the superuser. sudo -s ulimit -s unlimited Export the library path export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH Run the ungrib ./ungrib.exe You will see the successful output printed out at the completion of ungrib.exe and four output with prefix FILE are created. screenshot successful output screenshot four output of prefix FILE Remember to exit the superuser by typing exit. exit *Alternatives to download GFS data","title":"UNGRIB"},{"location":"ungrib/#ungrib","text":"Create GFS data under /WRF_Resources that you just created. cd /shared/scratch/WPS/WRF_Resources mkdir GFS_Data cd GFS_Data Website description, data and check if your dates are available. Create a python script to download the GFS Data. Make necessary changes on the dates and hours.This python script downloads 4 data on 20210629 with 6 hours interval. The alternatives would be going to the official Research Data Archive website : description and upload the python script through DCV. cat <<EOF > download_20210629.py #!/usr/bin/env python ################################################################# # Python Script to retrieve 4 online Data files of 'ds084.1', # total 2.06G. This script uses 'requests' to download data. # # Highlight this script by Select All, Copy and Paste it into a file; # make the file executable and run it on command line. # # You need pass in your password as a parameter to execute # this script; or you can set an environment variable RDAPSWD # if your Operating System supports it. # # Contact rdahelp@ucar.edu (RDA help desk) for further assistance. ################################################################# import sys, os import requests def check_file_status(filepath, filesize): sys.stdout.write('\\r') sys.stdout.flush() size = int(os.stat(filepath).st_size) percent_complete = (size/filesize)*100 sys.stdout.write('%.3f %s' % (percent_complete, '% Completed')) sys.stdout.flush() # Try to get password if len(sys.argv) < 2 and not 'RDAPSWD' in os.environ: try: import getpass input = getpass.getpass except: try: input = raw_input except: pass pswd = input('Password: ') else: try: pswd = sys.argv[1] except: pswd = os.environ['RDAPSWD'] url = 'https://rda.ucar.edu/cgi-bin/login' values = {'email' : 'YOUR EMAIL', 'passwd' : pswd, 'action' : 'login'} # Authenticate ret = requests.post(url,data=values) if ret.status_code != 200: print('Bad Authentication') print(ret.text) exit(1) dspath = 'https://rda.ucar.edu/data/ds084.1/' filelist = [ '2021/20210629/gfs.0p25.2021062900.f000.grib2', '2021/20210629/gfs.0p25.2021062906.f000.grib2', '2021/20210629/gfs.0p25.2021062912.f000.grib2', '2021/20210629/gfs.0p25.2021062918.f000.grib2'] for file in filelist: filename=dspath+file file_base = os.path.basename(file) print('Downloading',file_base) req = requests.get(filename, cookies = ret.cookies, allow_redirects=True, stream=True) filesize = int(req.headers['Content-length']) with open(file_base, 'wb') as outfile: chunk_size=1048576 for chunk in req.iter_content(chunk_size=chunk_size): outfile.write(chunk) if chunk_size < filesize: check_file_status(file_base, filesize) check_file_status(file_base, filesize) print() EOF Execute the python script to download the data. You must be a registered user on the NCAR website because you will be asked for password before the permission to download the data is granted. pip3 install requests python3 download_20210629.py Obtain the path to your GFS_Data where in this case and go back to the directory where your WPS is compiled and create the symbolic link to the GFS data that you just created. You should expect four GRIBFILE with same prefix but different labels behind, AAA, AAB,AAC and AAD. cd /shared/scratch/WPS ./link_grib.csh /shared/scratch/WPS/WRF_Resources/GFS_Data/gfs.* *screenshot for the four output GRIBFILE.AAA Create the symbolic link to the Vtable. ln -sf ungrib/Variable_Tables/Vtable.GFS Vtable *screenshot for the Vtable Edit the namelist.wps. start date end date the interval seconds that desribe the interval between your GFS data -- nano namelist.wps Change the domain to 1 or 2 according to your needs. Increase your stack limit as the superuser. sudo -s ulimit -s unlimited Export the library path export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH Run the ungrib ./ungrib.exe You will see the successful output printed out at the completion of ungrib.exe and four output with prefix FILE are created. screenshot successful output screenshot four output of prefix FILE Remember to exit the superuser by typing exit. exit *Alternatives to download GFS data","title":"UNGRIB"},{"location":"wps/","text":"WPS Installation cat <<EOF > wps-install.sbatch #!/bin/bash #SBATCH -N 1 #SBATCH --exclusive spack install -j 1 wps%intel ^intel-oneapi-mpi+external-libfabric EOF sbatch wps-install.sbatch Checking the Clusters squeue cat slurm-<jobID>.out -- or tail slurm-<jobID>.out spack find","title":"WPS Installation"},{"location":"wps/#wps-installation","text":"cat <<EOF > wps-install.sbatch #!/bin/bash #SBATCH -N 1 #SBATCH --exclusive spack install -j 1 wps%intel ^intel-oneapi-mpi+external-libfabric EOF sbatch wps-install.sbatch","title":"WPS Installation"},{"location":"wps/#checking-the-clusters","text":"squeue cat slurm-<jobID>.out -- or tail slurm-<jobID>.out spack find","title":"Checking the Clusters"},{"location":"wps_configuration/","text":"WPS Configuration Create a soft link for the WPS spack load intel-oneapi-compilers spack load intel-oneapi-mpi COPY INSTEAD OF USING THE SOFT LINK cd /shared/scratch mkdir WPS cp -a $(spack location -i wps%intel)/. WPS/ cd WPS ls Jasper Installation spack install jasper@2.0.31%intel -- spack find Export export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64 export JASPERINC=$(spack location -i jasper@2.0.31%intel)/include export WRF_DIR=$(spack location -i wrf%intel) export NETCDFINC=$(spack location -i netcdf-fortran%intel)/include export NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib export NETCDF=$(spack location -i netcdf-fortran%intel) export NETCDFF=$(spack location -i netcdf-fortran%intel) -- echo $NETCDFINC echo $NETCDFLIB -- cp -a $(spack location -i netcdf-c%intel)/include/. $(spack location -i netcdf-fortran%intel)/include/ cp -a $(spack location -i netcdf-c%intel)/lib/. $(spack location -i netcdf-fortran%intel)/lib/ -- Edit configure.wps File ./configure Type Y Then paste the NETCDFINC path , ... nano configure.wps Add -lnetcdff -lnetcdf -qopenmp then save -- Compile ./compile > log.compile Clean - to reset WPS settings! ./clean -a","title":"WPS Configuration"},{"location":"wps_configuration/#wps-configuration","text":"","title":"WPS Configuration"},{"location":"wps_configuration/#create-a-soft-link-for-the-wps","text":"spack load intel-oneapi-compilers spack load intel-oneapi-mpi COPY INSTEAD OF USING THE SOFT LINK cd /shared/scratch mkdir WPS cp -a $(spack location -i wps%intel)/. WPS/ cd WPS ls","title":"Create a soft link for the WPS"},{"location":"wps_configuration/#jasper-installation","text":"spack install jasper@2.0.31%intel -- spack find","title":"Jasper Installation"},{"location":"wps_configuration/#export","text":"export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64 export JASPERINC=$(spack location -i jasper@2.0.31%intel)/include export WRF_DIR=$(spack location -i wrf%intel) export NETCDFINC=$(spack location -i netcdf-fortran%intel)/include export NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib export NETCDF=$(spack location -i netcdf-fortran%intel) export NETCDFF=$(spack location -i netcdf-fortran%intel) -- echo $NETCDFINC echo $NETCDFLIB -- cp -a $(spack location -i netcdf-c%intel)/include/. $(spack location -i netcdf-fortran%intel)/include/ cp -a $(spack location -i netcdf-c%intel)/lib/. $(spack location -i netcdf-fortran%intel)/lib/ --","title":"Export"},{"location":"wps_configuration/#edit-configurewps-file","text":"./configure Type Y Then paste the NETCDFINC path , ... nano configure.wps Add -lnetcdff -lnetcdf -qopenmp then save --","title":"Edit configure.wps File"},{"location":"wps_configuration/#compile","text":"./compile > log.compile Clean - to reset WPS settings! ./clean -a","title":"Compile"}]}